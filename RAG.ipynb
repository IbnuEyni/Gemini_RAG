{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "os.environ[\"GEMINI_API_KEY\"]=st.secrets[\"GEMINI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Reads the text content from a PDF file and returns it as a single string.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The file path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    - str: The concatenated text content of all pages in the PDF.\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If the specified file_path does not exist.\n",
    "    - PyPDF2.utils.PdfReadError: If the PDF file is encrypted or malformed.\n",
    "\n",
    "    Example:\n",
    "    >>> pdf_text = load_pdf(\"example.pdf\") \n",
    "    >>> print(pdf_text)\n",
    "    \"This is the text content extracted from the PDF file.\"\n",
    "    \"\"\"\n",
    "    # Logic to read pdf\n",
    "    reader = PdfReader(file_path)\n",
    "\n",
    "    # Loop over each page and store it in a variable\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "\n",
    "    return text\n",
    "\n",
    "text = load_pdf(file_path=\"/home/shuaib/Desktop/School/AI_programming/Gemini_RAG/data/state_of_the_union.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text = load_pdf(file_path=\"/home/shuaib/Desktop/School/AI_programming/Gemini_RAG/data/state_of_the_union.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to split the text into chunks\n",
    "def split_text(text: str, chunk_size: int = 10000, chunk_overlap: int = 500):\n",
    "    \"\"\"\n",
    "    Splits a text string into smaller chunks based on sentence boundaries while ensuring\n",
    "    that each chunk does not exceed a specified size. The function also allows for a \n",
    "    slight overlap between chunks to maintain context.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input text to be split.\n",
    "    - chunk_size (int): The maximum size of each chunk in characters (default is 10000).\n",
    "    - chunk_overlap (int): The number of overlapping characters between consecutive chunks (default is 500).\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: A list containing text chunks that do not exceed the specified chunk size.\n",
    "    \"\"\"\n",
    "    # Split text into sentences using regular expressions\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    # Iterate over sentences and build chunks\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) < chunk_size:\n",
    "            current_chunk += \" \" + sentence\n",
    "        else:\n",
    "            # Add the current chunk to the list and start a new chunk\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "\n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_text = split_text(pdf_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "import os\n",
    "\n",
    "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "    \"\"\"\n",
    "    Custom embedding function using the Gemini AI API for document retrieval.\n",
    "\n",
    "    This class extends the EmbeddingFunction class and implements the __call__ method\n",
    "    to generate embeddings for a given set of documents using the Gemini AI API.\n",
    "\n",
    "    Parameters:\n",
    "    - input (Documents): A collection of documents to be embedded.\n",
    "\n",
    "    Returns:\n",
    "    - Embeddings: Embeddings generated for the input documents.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If the Gemini API Key is not provided as an environment variable (GEMINI_API_KEY).\n",
    "\n",
    "    Example:\n",
    "    >>> gemini_embedding_function = GeminiEmbeddingFunction()\n",
    "    >>> input_documents = Documents([\"Document 1\", \"Document 2\", \"Document 3\"])\n",
    "    >>> embeddings_result = gemini_embedding_function(input_documents)\n",
    "    >>> print(embeddings_result)\n",
    "    Embeddings for the input documents generated by the Gemini AI API.\n",
    "    \"\"\"\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "        if not gemini_api_key:\n",
    "            raise ValueError(\"Gemini API Key not provided. Please provide GEMINI_API_KEY as an environment variable\")\n",
    "        genai.configure(api_key=gemini_api_key)\n",
    "        model = \"models/embedding-001\"\n",
    "        title = \"Custom query\"\n",
    "        return genai.embed_content(model=model,\n",
    "                                   content=input,\n",
    "                                   task_type=\"retrieval_document\",\n",
    "                                   title=title)[\"embedding\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing vectors into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from typing import List\n",
    "def create_chroma_db(documents:List, path:str, name:str):\n",
    "    \"\"\"\n",
    "    Creates a Chroma database using the provided documents, path, and collection name.\n",
    "\n",
    "    Parameters:\n",
    "    - documents: An iterable of documents to be added to the Chroma database.\n",
    "    - path (str): The path where the Chroma database will be stored.\n",
    "    - name (str): The name of the collection within the Chroma database.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[chromadb.Collection, str]: A tuple containing the created Chroma Collection and its name.\n",
    "    \"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=path)\n",
    "    db = chroma_client.create_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n",
    "\n",
    "    for i, d in enumerate(documents):\n",
    "        db.add(documents=d, ids=str(i))\n",
    "\n",
    "    return db, name\n",
    "\n",
    "db,name =create_chroma_db(documents=chunked_text, \n",
    "                          path=\"/home/shuaib/Desktop/School/AI_programming/RAG/content\",\n",
    "                          name=\"rag_experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Function to create or load ChromaDB and store vectors\n",
    "def create_or_load_chroma_db(documents, path: str, name: str):\n",
    "    \"\"\"\n",
    "    Creates or loads a Chroma database collection using the provided documents, path, and collection name.\n",
    "    If the collection already exists, it will be loaded; otherwise, a new collection will be created.\n",
    "\n",
    "    Parameters:\n",
    "    - documents: An iterable of documents to be added to the Chroma database if creating a new collection.\n",
    "    - path (str): The path where the Chroma database will be stored or accessed.\n",
    "    - name (str): The name of the collection within the Chroma database.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[chromadb.Collection, str]: A tuple containing the Chroma Collection and its name.\n",
    "    \"\"\"\n",
    "    # Initialize ChromaDB client with the specified path\n",
    "    chroma_client = chromadb.PersistentClient(path=path)\n",
    "    \n",
    "    # Get the names of existing collections\n",
    "    existing_collections = chroma_client.list_collections()\n",
    "    collection_names = [collection.name for collection in existing_collections]\n",
    "\n",
    "    # Check if the collection already exists\n",
    "    if name in collection_names:\n",
    "        st.write(f\"Loading existing collection: {name}\")\n",
    "        # Load the existing collection\n",
    "        db = chroma_client.get_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n",
    "    else:\n",
    "        st.write(f\"Creating new collection: {name}\")\n",
    "        # Create a new collection and add the documents\n",
    "        db = chroma_client.create_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n",
    "        for i, d in enumerate(documents):\n",
    "            db.add(documents=d, ids=str(i))\n",
    "    \n",
    "    return db, name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-04 10:57:03.519 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /home/shuaib/.local/lib/python3.10/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "db,name =create_or_load_chroma_db(documents=text, path=\"/home/shuaib/Desktop/School/AI_programming/RAG/content\", name=\"rag_experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_passage(query: str, db, n_results: int):\n",
    "    \"\"\"\n",
    "    Retrieves the most relevant passage from the database based on the provided query.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The search query to find the relevant passage.\n",
    "    - db: The Chroma database collection to query.\n",
    "    - n_results (int): The number of results to retrieve from the query.\n",
    "\n",
    "    Returns:\n",
    "    - str: The most relevant passage based on the query, or an empty string if no relevant passages are found.\n",
    "    \"\"\"\n",
    "    # Query the database with the given search query and number of results\n",
    "    results = db.query(query_texts=[query], n_results=n_results)\n",
    "    \n",
    "    # Extract the most relevant passage from the query results\n",
    "    # If there are no documents found, return an empty string\n",
    "    passage = results['documents'][0] if results['documents'] else \"\"\n",
    "    \n",
    "    return passage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    }
   ],
   "source": [
    "relevant_text = get_relevant_passage(\"Sanctions on Russia\",db,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "def generate_response(query: str, relevant_passages: list):\n",
    "    \"\"\"\n",
    "    Generates a detailed response to a query based on provided relevant passages using the Gemini AI model.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The question or query that needs to be answered.\n",
    "    - relevant_passages (list): A list of relevant passages or context to use for generating the response.\n",
    "\n",
    "    Returns:\n",
    "    - str: The generated response from the Gemini AI model.\n",
    "    \"\"\"\n",
    "    # Retrieve the Gemini API key from environment variables\n",
    "    gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not gemini_api_key:\n",
    "        raise ValueError(\"Gemini API Key not provided. Please provide GEMINI_API_KEY as an environment variable\")\n",
    "    \n",
    "    # Configure the generative AI model with the provided API key\n",
    "    genai.configure(api_key=gemini_api_key)\n",
    "    model = genai.GenerativeModel('gemini-pro')\n",
    "    \n",
    "    # Combine relevant passages into a single context block\n",
    "    context = \"\\n\\n\".join(relevant_passages)\n",
    "    \n",
    "    # Create a prompt template that includes the context and the query\n",
    "    prompt_template = f\"\"\"\n",
    "    Answer the question as detailed as possible from the provided context,\n",
    "    making sure to provide all the details. If the answer is not in the provided context, \n",
    "    just say, \"The answer is not available in the context.\"\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question:\n",
    "    {query}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the content using the AI model based on the prompt template\n",
    "    answer = model.generate_content(prompt_template)\n",
    "    \n",
    "    return answer.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bringing it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_answer(db,query    ):\n",
    "    #retrieve top 3 relevant text chunks\n",
    "    relevant_text = get_relevant_passage(query,db,n_results=3)\n",
    "    answer = generate_response(query, relevant_text)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m db, _ \u001b[38;5;241m=\u001b[39mcreate_or_load_chroma_db(documents\u001b[38;5;241m=\u001b[39mtext, path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mRepos\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mRAG\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mconents\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrag_experiment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhat sanctions have been placed on Russia\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "Cell \u001b[0;32mIn[69], line 4\u001b[0m, in \u001b[0;36mgenerate_answer\u001b[0;34m(db, query)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_answer\u001b[39m(db,query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat sanctions have been placed on Russia\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m#retrieve top 3 relevant text chunks\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     relevant_text \u001b[38;5;241m=\u001b[39m get_relevant_passage(query,db,n_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelevant_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m answer\n",
      "Cell \u001b[0;32mIn[68], line 45\u001b[0m, in \u001b[0;36mgenerate_response\u001b[0;34m(query, relevant_passages)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Generate the content using the AI model based on the prompt template\u001b[39;00m\n\u001b[1;32m     43\u001b[0m answer \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate_content(prompt_template)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43manswer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/generativeai/types/generation_types.py:436\u001b[0m, in \u001b[0;36mBaseGenerateContentResponse.text\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    434\u001b[0m parts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparts\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parts:\n\u001b[0;32m--> 436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    439\u001b[0m     )\n\u001b[1;32m    441\u001b[0m texts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m parts:\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked."
     ]
    }
   ],
   "source": [
    "db, _ =create_or_load_chroma_db(documents=text, path=\"/home/shuaib/Desktop/School/AI_programming/Gemini_RAG/content\", name=\"rag_experiment\")\n",
    "answer = generate_answer(db, query=\"languages in Ethiopia\")\n",
    "print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
